---
layout: post
title: Flink SQL 调研实践总结
categories: [Flink,FlinkSQL]
description: 使用FlinkSQL1.10.0 对数据源进行读取操作,将结果集保存到不用数据集中.
keywords: Flink, FlinkSQL
---

## Flink SQL 调研实践总结

使用Flink 版本为:1.10.0
此版本中Flink SQL 对DDL语句支持直接创建Table方式使用.

注意: 基本使用能够,如果想利用Table方式对Hbase进行类似table查询读取操作失败,所以如果想使用这种方式的话,建议等到后续版本升级可能会支持.

### 本次调研实践的各个软件版本说明

Flink:1.10.0

Kafka:2.2.0

ES:7.2

HBase:2.1.0

Hadoop:3.0.0

JDK:1.8.0

scala:2.11.12 | pom中打包使用的是 2.12

### 使用情景:

source: kafka

sink: elesticsearch7, hbase

- 业务需求:
 
  利用 FlinkSQL 将 kafka中用户日志数据和hbase中数据数据关联查询.得到结果集.然后再存入hbase和es中.

####  具体代码实现

逻辑说明: source接入Kafka,中间经过统计,最后将数据写入到es中.

直接贴代码吧.
``` scala

import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.api.scala._
import org.apache.flink.runtime.state.filesystem.FsStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig.ExternalizedCheckpointCleanup
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.scala.{StreamTableEnvironment, _}
import org.apache.flink.types.Row

/**
 * 此程序没有做到实时更新当前小时.待优化.(此为FlinkSql的特性所致.虽然没有实时写入到结果集.但是中间计算是实时计算着走的.到窗口结束时会将结果直接写入到结果集中)
 */
object FlinkSqlMidKafkaSinkES extends ConfigBase {

  var sourSql =
    """
      |create table shafa_mid_log_json (
      | ext String,
      | os String,
      | agent_id String,
      | openid String,
      | ctx String,
      | ip String,
      | network String,
      | uid String,
      | admin_id String,
      | host String,
      | referral_id String,
      | theme String,
      | tag String,
      | page String,
      | track String,
      | channel_id String,
      | map_json String,
      | vt String,
      | ua String,
      | domain String,
      | reqtime BIGINT,
      | ts AS TO_TIMESTAMP(FROM_UNIXTIME(reqtime/1000,'yyyy-MM-dd HH:mm:ss')), -- flink 官方版本暂不支持 传入 bigint的类型.阿里云文档中可以.
      | proctime as PROCTIME(),   -- 通过计算列产生一个处理时间列
      | WATERMARK FOR ts as ts - INTERVAL '5' SECOND  -- 在et上定义watermark，et成为事件时间列
      | )
      | WITH (
      |'connector.type' = 'kafka',
      |'connector.version' = 'universal',
      |'connector.topic' = 'mid_shafa_json_log',
      |'connector.startup-mode' = 'latest-offset',
      |'connector.properties.group.id' = 'flink_sql_consume_mid_json',
      |'connector.properties.zookeeper.connect' = '%s',
      |'connector.properties.bootstrap.servers' = '%s',
      |'format.type' = 'json',
      |'update-mode'='append'
      | )""".stripMargin

  var sinkSql =
    s"""CREATE TABLE sink_es_shafa_pv_uv (
       |ts  STRING,
       |channel_id String,
       |pv BIGINT,
       |uv BIGINT
       |) WITH (
       |'connector.type' = 'elasticsearch',
       |'connector.version' = '7',
       |'connector.hosts' = '%s',
       |'connector.index' = 'shafa_pv_uv_dh',
       |'connector.document-type' = 'shafa_pv_uv',
       |'connector.bulk-flush.max-actions' = '1',
       |'format.type' = 'json',
       |'update-mode' = 'upsert'
       |)""".stripMargin

  def main(args: Array[String]): Unit = {
    val paramTool = getProperties(args)
    val zookeeperQuorum = paramTool.get("zookeeper.quorum")
    val esHttpHost = paramTool.get("es.http.host")
    val brokerServers = paramTool.get("kafka.bootstrap.servers")
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    //启用并设置checkpoint相关属性
    env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE)
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)
    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)
    env.getCheckpointConfig.setCheckpointTimeout(60000)
    //2049 9870
    //    env.setStateBackend(new FsStateBackend("hdfs://hd-master-101:9000/flink/checkpoints/shafa_pv_uv/"));
    env.setStateBackend(new FsStateBackend("hdfs:///flink/checkpoints/shafa_pv_uv/"));
    env.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)
    //重启策略
    env.setRestartStrategy(RestartStrategies.noRestart())


    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    env.setParallelism(1)

    val tEnv = StreamTableEnvironment.create(env, settings)
    //origin_cps_shafa_logs
    tEnv.sqlUpdate(sourSql.format(zookeeperQuorum, brokerServers))
    //    tEnv.sqlUpdate(sourConsumeSql)
    tEnv.sqlUpdate(sinkSql.format(esHttpHost))
    //    tEnv.sqlQuery("""select channel_id,uid,ts,proctime from shafa_mid_log_json """).toAppendStream[Row].print()
    //统计一分钟内的数据
    tEnv.sqlQuery(
      """
        |
        |select
        | DATE_FORMAT(TUMBLE_START(ts, interval '10' minute),'yyyyMMddHHmm') as ts,
        | 'all' as channel_id,
        | count(uid) as pv,
        | count(distinct uid) as pv
        | from shafa_mid_log_json
        | group by tumble(ts,interval '10' minute )
        |""".stripMargin)
      .toAppendStream[Row].print()

    //    //5分钟
    //    tEnv.sqlUpdate(
    //      """
    //        |insert into sink_es_shafa_pv_uv
    //        |select
    //        | DATE_FORMAT(TUMBLE_END(ts, interval '5' minute),'yyyyMMddHHmm') as ts,
    //        | 'all' as channel_id,
    //        | count(uid) as pv,
    //        | count(distinct uid) as pv
    //        | from shafa_mid_log_json
    //        | group by tumble(ts,interval '5' minute )
    //        |""".stripMargin)

    //统计一个小时内pv,uv
    tEnv.sqlUpdate(
      """
        |insert into sink_es_shafa_pv_uv
        | select
        | DATE_FORMAT(TUMBLE_START(ts, interval '1' hour),'yyyyMMddHH') as ts,
        | channel_id,
        | count(uid) as pv,
        | count(distinct uid) as pv
        | from shafa_mid_log_json
        | group by tumble(ts,interval '1' hour ) , channel_id
        |""".stripMargin)

    //统计当前小时总的pv,uv
    tEnv.sqlUpdate(
      """
        |insert into sink_es_shafa_pv_uv
        | select
        | DATE_FORMAT(TUMBLE_START(ts, interval '1' hour),'yyyyMMddHH') as ts,
        | 'hour' as channel_id,
        | count(uid) as pv,
        | count(distinct uid) as pv
        | from shafa_mid_log_json
        | group by tumble(ts,interval '1' hour )
        |""".stripMargin)

    //统计当天每个渠道总的pv,uv
    tEnv.sqlUpdate(
      """
        |insert into sink_es_shafa_pv_uv
        | select
        | DATE_FORMAT(TUMBLE_START(ts, interval '1' day),'yyyyMMdd') as ts,
        | channel_id,
        | count(uid) as pv,
        | count(distinct uid) as pv
        | from shafa_mid_log_json
        | group by tumble(ts,interval '1' day ),channel_id
        |""".stripMargin)

    //统计当天总的pv,uv
    tEnv.sqlUpdate(
      """
        |insert into sink_es_shafa_pv_uv
        | select
        | DATE_FORMAT(TUMBLE_START(ts, interval '1' day),'yyyyMMdd') as ts,
        | 'day' as channel_id,
        | count(uid) as pv,
        | count(distinct uid) as pv
        | from shafa_mid_log_json
        | group by tumble(ts,interval '1' day )
        |""".stripMargin)

    env.execute("stat_shafa_pv_uv_es")
  }

}

```

### 总结: 
1. kafka接入时利用了计算列
    ``` scala
        | reqtime BIGINT,
        | ts AS TO_TIMESTAMP(FROM_UNIXTIME(reqtime/1000,'yyyy-MM-dd HH:mm:ss')) 
    ```
flink sql1.10.0 中TO_TIMESTAMP函数还不支持时间戳(整型数据),支持符合时间格式的字符串形式.但是阿里云文档的支持.

2. 写入结果集需等到窗口结束时一次性写入.

3. 目前table 对hbase只能做写入操作.对读取操作不友好,测试发现暂时无法正常使用.不建议使用table方式读取Hbase,但是可以做写入.Flink SQL后续版本可能会升级.

具体代码测试读取Hbase:
``` scala

import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}
import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.scala.{StreamTableEnvironment, _}
import org.apache.flink.types.Row
/**
 * hbase join flink sql table test
 */
object HbaseJoinFlinkTableV2 extends ConfigBase {
  var sourceSql =
    """
      |create table cps_hive_to_hbase (
      | id VARCHAR,
      | cf1 ROW(
      | user_id VARCHAR,
      | channel_id VARCHAR,
      | open_id VARCHAR,
      | book_id VARCHAR,
      | book_sex VARCHAR,
      | book_category_id VARCHAR,
      | read_chapter_num VARCHAR,
      | all_read_chapter_num VARCHAR,
      | utime VARCHAR,
      | rgdt VARCHAR,
      | sex VARCHAR,
      | subscribe_time VARCHAR,
      | vip_endtime VARCHAR,
      | update_user_time VARCHAR,
      | is_pay VARCHAR,
      | cz_money VARCHAR,
      | cz_count VARCHAR,
      | all_xf_free_kandian VARCHAR,
      | all_xf_money VARCHAR,
      | all_remain_money VARCHAR,
      | all_remain_free_money VARCHAR
      | )
      |
      | )
      | WITH (
      |'connector.type' = 'hbase',
      |'connector.version' = '1.4.3',
      |'connector.table-name' = 'cps_hive_to_hbase',
      |'connector.zookeeper.znode.parent' = '/hbase',
      |'connector.zookeeper.quorum' = '%s',
      |'connector.write.buffer-flush.max-size' = '10mb',
      |'connector.write.buffer-flush.max-rows' = '1000',
      |'connector.write.buffer-flush.interval' = '2s'
      | )""".stripMargin

  //
  def main(args: Array[String]): Unit = {
    val paramTool = getProperties(args)
    val zookeeper = paramTool.get("zookeeper.quorum")
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    //    env.setParallelism(1)
    val tEnv = StreamTableEnvironment.create(env, settings)
    sourceSql = String.format(sourceSql, zookeeper)

    println(sourceSql)
    tEnv.sqlUpdate(sourceSql)

    val query = tEnv.sqlQuery("select * from  cps_hive_to_hbase limit 10 ").toAppendStream[Row].print()

    env.execute("test hbase")

  }
}

```
会抛出异常:
``` log
Exception in thread "main" java.lang.RuntimeException: java.util.concurrent.ExecutionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/dispatcher#-523925282]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.


```
查看官方论坛[issues](https://issues.apache.org/jira/browse/FLINK-11143)中有人提及,未找到解决方案.


### 附录一 代码

``` scala

import org.apache.flink.api.java.utils.ParameterTool

class ConfigBase {

  def getProperties(args: Array[String]): ParameterTool = {
    ParameterTool.fromPropertiesFile(this.getClass.getClassLoader.getResourceAsStream("application.yml"))
  }

}

```
### 附录二 POM.xml

``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>dz-rt</artifactId>
        <groupId>com.dz.rt</groupId>
        <version>1.0.0</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>flink-sql</artifactId>

    <properties>
        <flink.version>1.10.0</flink.version>
        <scala.library.verison>2.12</scala.library.verison>
        <scala.version>2.12.11</scala.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-core</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-scala_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
            <!--            <scope>provided</scope>-->
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
            <!--            <scope>provided</scope>-->
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-elasticsearch7_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-scala-bridge_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-blink_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>


        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-common</artifactId>
            <version>${flink.version}</version>
            <!--            <scope>provided</scope>-->
        </dependency>


        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table</artifactId>
            <version>${flink.version}</version>
            <type>pom</type>
            <!--            <scope>provided</scope>-->
        </dependency>


        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.68</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-scala_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>

        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-json</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-csv</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.flink/flink-hbase -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hbase_${scala.library.verison}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>3.0.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>3.0.0</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.1.0</version>
<!--            <version>1.4.3</version>-->
        </dependency>

    </dependencies>

    <build>
        <resources>
            <resource>
                <directory>${basedir}/src/main/scala/</directory>
                <targetPath>target/classes/</targetPath>
                <includes>
                    <include>*.*</include>
                </includes>
            </resource>
            <resource>
                <directory>${basedir}/src/main/resources/</directory>
                <includes>
                    <include>*.*</include>
                </includes>
            </resource>
        </resources>
        <plugins>
            <!-- Scala Compiler -->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <args>
                        <arg>-nobootcp</arg>
                    </args>
                </configuration>
            </plugin>

            <!-- 编译插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.7.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <executions>
                    <execution>
                        <id>copy-dependencies</id>
                        <phase>prepare-package</phase>
                        <goals>
                            <goal>copy-dependencies</goal>
                        </goals>
                        <configuration>
                            <outputDirectory>${project.build.directory}/lib</outputDirectory>
                            <overWriteReleases>false</overWriteReleases>
                            <overWriteSnapshots>false</overWriteSnapshots>
                            <overWriteIfNewer>true</overWriteIfNewer>
                        </configuration>
                    </execution>
                </executions>
            </plugin>


        </plugins>
    </build>
</project>
```