---
layout: post
title: flume的搭建和简单使用
categories: [Flume]
description: flume的搭建和简单使用
keywords: flume搭建, flume使用
---

### flume 简介:
- 百度百科:<br>
    Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。
    当前Flume有两个版本Flume 0.9X版本的统称Flume-og，Flume1.X版本的统称Flume-ng。由于Flume-ng经过重大重构，与Flume-og有很大不同，使用时请注意区分。
- 简介2<br>
    1) Flume 提供一个分布式的，可靠的，对大数据量的日志进行高效收集、聚集、移动的服务， Flume 只能在 Unix 环境下运行。<br>
    2) Flume 基于流式架构，容错性强，也很灵活简单。<br>
    3) Flume、Kafka 用来实时进行数据收集，Spark、Storm 用来实时处理数据，impala 用来实 时查询。<br>

--- 

### 模式
参考[官文文档](http://flume.apache.org/FlumeUserGuide.html)<br>
模式一:单输入单输出 <br>
    ![](https://stone-upyun.b0.aicdn.com/blog20180502120225.png!700x999)

模式二:多flume连接
    ![](https://stone-upyun.b0.aicdn.com/blog20180502120550.png!700x999)

模式三:多输入汇总
    ![](https://stone-upyun.b0.aicdn.com/blog20180502120652.png!700x999)

模式四:多输出分发
    ![](https://stone-upyun.b0.aicdn.com/blog20180502120712.png!700x999)

---

### Flume 角色<br>
- 1、Source <br>
    用于采集数据，Source 是产生数据流的地方，同时 Source 会将产生的数据流传输到 Channel，
    这个有点类似于 Java IO 部分的 Channel。<br>
- 2、Channel<br>
    用于桥接 Sources 和 Sinks，类似于一个队列。<br>
- 3、Sink<br>
    从 Channel 收集数据，将数据写到目标源(可以是下一个 Source，也可以是 HDFS 或者 HBase)。
- 4、Event<br>
    传输单元，Flume 数据传输的基本单元，以事件的形式将数据从源头送至目的地

### 安装

下载官方对应的安装包,解压<br>
修改 conf/flume-env.sh 文件中JAVA_HOME即可使用<br>
本文使用的版本:1.7.0 下载地址: [官方下载地址](http://archive.apache.org/dist/flume/)

### 遇到的问题:
在flume中配置两个sink输出到指定两端口,然后用两个soureces去接受这两个sink传递过来的值,,最终将之前其中一个sink的发送的数据放到hdfs,另一个数据放到本地磁盘.但是放到本地磁盘的那份数据,只有目录和文件,数据没有,放到hdfs上的有数据<br>
第一个flume的配置
``` 
# 讲一个source 分发给多个chanel-对应多个sink，给下一个相应的flume
a1.sources = r1
a1.channels = c1 c2
a1.sinks = k1 k2

# 将数据复制给多个channel
a1.sources.r1.selector.type = replicating

# sources 设置
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/admin/module/hive-1.2.2/logs/hive.log
a1.sources.r1.shell = /bin/bash -c

#channels 设置
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# sinks 设置
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = hd001
a1.sinks.k2.port = 8088
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = hd001
a1.sinks.k1.port = 8089  
```

第二个flume的配置
```
#接受上一个flume 的sink的结果# 定义服务
a2.sources = r1
a2.channels = c1
a2.sinks = k1

# source 设置
a2.sources.r1.type = avro
a2.sources.r1.bind = hd001
a2.sources.r1.port = 8089

# sink 设置
a2.sinks.k1.type = hdfs
a2.sinks.k1.hdfs.path = hdfs://192.168.1.20:9000/flume4/manysinks/%Y%m%d/%H
#上传文件的前缀
a2.sinks.k1.hdfs.filePrefix = upload2-
#是否按照时间滚动文件夹
a2.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a2.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
a2.sinks.k1.hdfs.roundUnit = hour
#是否使用本地时间戳
a2.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a2.sinks.k1.hdfs.batchSize = 1000
#设置文件类型，可支持压缩
a2.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a2.sinks.k1.hdfs.rollInterval = 600
#设置每个文件的滚动大小
a2.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a2.sinks.k1.hdfs.rollCount = 0
#最小冗余数
a2.sinks.k1.hdfs.minBlockReplicas = 1


# Use a channel which buffers events in memory
a2.channels.c1.type = memory
a2.channels.c1.capacity = 1000
a2.channels.c1.transactionCapacity = 100

a2.sources.r1.channels = c1
a2.sinks.k1.channel = c1

a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2 
```

第三个flume的配置  
```
#接受上一个flume 的sink的结果# 定义服务
a3.sources = r1
a3.channels = c1
a3.sinks = k1
# source 设置
a3.sources.r1.type = avro
a3.sources.r1.bind = hd001
a3.sources.r1.port = 8088
# sink 设置
a3.sinks.k1.type = file_roll
a3.sinks.k1.sink.directory = /home/admin/tmp/flume4
# Use a channel which buffers events in memory
a3.channels.c1.type = memory
a3.channels.c1.capacity = 100
a3.channels.c1.transactionCapacity = 50

a3.sources.r1.cahnnels = c1
a3.sinks.k1.channel = c1  
```
查看第三个flume配置中对应结果目录数据
```
[admin@hd001 ~]$ ll tmp/flum4/
总用量 0
-rw-rw-r--. 1 admin admin 0 5月   2 11:32 1525231977673-1
-rw-rw-r--. 1 admin admin 0 5月   2 11:37 1525231977673-10
-rw-rw-r--. 1 admin admin 0 5月   2 11:37 1525231977673-11
-rw-rw-r--. 1 admin admin 0 5月   2 11:38 1525231977673-12
-rw-rw-r--. 1 admin admin 0 5月   2 11:39 1525231977673-13
-rw-rw-r--. 1 admin admin 0 5月   2 11:39 1525231977673-14
-rw-rw-r--. 1 admin admin 0 5月   2 11:39 1525231977673-15
-rw-rw-r--. 1 admin admin 0 5月   2 11:40 1525231977673-16
-rw-rw-r--. 1 admin admin 0 5月   2 11:41 1525231977673-17
-rw-rw-r--. 1 admin admin 0 5月   2 11:41 1525231977673-18
-rw-rw-r--. 1 admin admin 0 5月   2 11:41 1525231977673-19
里面的数据为0,cat * 出来也是空的,但是在第二个flume中到hdfs上的数据是有对应的hive.log日志

```
