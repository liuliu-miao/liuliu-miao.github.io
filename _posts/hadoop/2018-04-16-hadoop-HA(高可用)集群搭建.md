---
layout: post
title: hadoop-HA(高可用)集群搭建
categories: [Hadoop,Zookeeper]
description: hadoop-HA(高可用,high-avaliability)集群搭建
keywords: hadoop-HA, hadoop高可用
---

#### 准备工作
- 静态IP<br>
- 映射主机名与ip<br>
- jdk环境<br>
- 防火墙关闭<br>
- ssh免密<br>

#### 高可用原理介绍
- ##### 为何使用HA<br>
因为在非HA集群模式下namenode是单节点工作,故存在namenode的单节点故障.<br>
为防止故障发生,需要使用多个namenode节点.<br>
其中,hadoop2.x版本只能支持2个namenode节点 ,hadoop3.x可以支持更多个<br>
本文以hadoop2.7.2版本为基础
<br>


- ##### HA实现原理    
两个namenode的数据是一致的.<br>
两个namenode通过hadoop中的journalnode集群服务来同步两个namenode的元数据<br>
两个namenode工作时,其中一台处于active状态,另外一台处于standy状态,否则会出现(split-brain)脑裂,集群无法工作.<br>
在故障自动转移中, 通过 zookeeper 中的ZKFC(zookeeper failover controller,zookeeper 故障转移控制)进程来控制namenode节点的工作状态切换<br>
其中ZKFC会在两个namenode节点只监听namenode的状态信息,不会对元数据进行操作,<br>
当处于active的namenode宕机或死掉,zookeeper会通过ZKFC通知另外一台namenode启用(active)


#### namenode的高可用配置
- ##### 1.解压或复制原来hadoop集群相关文件<br>

    ```
    tar -zxf hadoop-2.7.2.tar.gz -C ~/module/HA/
    ```

- ##### 2.配置hadoop相关文件

    ```
    cd /home/admin/module/HA/hadoop-2.7.2/etc/hadoop    
    ```
- ###### 编辑core-site.xml(此时为非自动故障转移)
    ```
    <configuration>
      <!-- 把两个NameNode）的地址组装成一个集群haCluster(haCluster为集群名称,后续需要跟此保持一致) -->
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://haCluster</value>
      </property>

      <!-- # dfs 系统存取数据的目录 -->
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/home/admin/module/HA/hadoop-2.7.2/data/tmp</value>
      </property>
      <!-- journalnode 数据存储目录 -->
      <property>
        <name>dfs.journalnode.edit.dir</name>
        <value>/home/admin/module/HA/hadoop-2.7.2/jn/haCluster</value>
      </property>
      <!-- zookeeper通信客户端地址  -->
      <!-- <property>
          <name>ha.zookeeper.quorum</name>
          <value>hd001:2181,hd002:2181,hd003:2181</value>
      </property>  -->

    </configuration>
    ```

- ###### 编辑hdfs-site.xml
    ```
    <configuration>
      <!-- 完全分布式集群名称 haCluster 此处的haCluster与core-site.xmlvs中的集群名称需要一致,此xml种的haCluster都是集群对应的名称 -->
      <property>
        <name>dfs.nameservices</name>
        <value>haCluster</value>
      </property>

      <!-- 集群中NameNode节点都有哪些 -->
      <property>
        <name>dfs.ha.namenodes.haCluster</name>
        <value>nn1,nn2</value>
      </property>

      <!-- nn1的RPC通信地址 -->
      <property>
        <name>dfs.namenode.rpc-address.haCluster.nn1</name>
        <value>hd002:9000</value>
      </property>

       <!-- nn2的RPC通信地址 -->
      <property>
        <name>dfs.namenode.rpc-address.haCluster.nn2</name>
        <value>hd003:9000</value>
      </property>

      <!-- nn1的http通信地址 -->
      <property>
        <name>dfs.namenode.http-address.haCluster.nn1</name>
        <value>hd002:50070</value>
      </property>

      <!-- nn2的http通信地址 -->
      <property>
        <name>dfs.namenode.http-address.haCluster.nn2</name>
        <value>hd003:50070</value>
      </property>

      <!-- 指定NameNode元数据在JournalNode上的存放位置 -->
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hd001:8485;hd002:8485;hd003:8485/haCluster</value>
      </property>
     <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
      </property>

      <!-- 使用隔离机制时需要ssh无秘钥登录-->
      <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/admin/.ssh/id_rsa</value>
      </property>

      <!-- 声明journalnode服务器存储目录-->
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/home/admin/module/HA/hadoop-2.7.2/jn</value>
      </property>

      <!-- 关闭权限检查-->
      <property>
        <name>dfs.permissions.enable</name>
        <value>false</value>
      </property>

      <!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式-->
      <property>
        <name>dfs.client.failover.proxy.provider.haCluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>

      <!-- # hdfs文件系统中的文件副本数量 为1(一般情况,完全分布式都是3分以上基数份) -->
      <property>
              <name>dfs.replication</name>
              <value>1</value>
      </property>

      <!-- # hdfs文件系统中的文件副本数量 为1(一般情况,完全分布式都是3分以上基数份) -->
      <property>
              <name>dfs.replication</name>
              <value>1</value>
      </property>

      <!-- # 节点检测频率,用户namenode 检测datanode是否存活 120s -->
      <property>
              <name>dfs.namenode.checkpoint.period</name>
              <value>120</value>
      </property>
      <!-- 启用web查看hdfs系统 -->
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <!-- 启用自动故障转移 -->
      <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
      </property>
      <!-- # 指定 dfs 相关的机器地址,用户上下线新的机器 -->
      <!-- <property>
              <name>dfs.hosts</name>
              <value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts</value>
      </property>
      # 指定退役的节点
      <property>
              <name>dfs.hosts.exclude</name>
              <value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude</value>
      </property> -->
    </configuration>
    ```

- ###### 编辑mapred-site.xml
    ```
    <configuration>
        <!-- 指定mr运行在yarn上 -->
          <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
          </property>

          <!--配置历史服务器 -->
          <property>
                  <name>mapreduce.jobhistory.address</name>
                  <value>hd001:10020</value>
          </property>
          <property>
                  <name>mapreduce.jobhistory.webapp.address</name>
                  <value>hd001:19888</value>
          </property>

    </configuration>
    ```
    将hadoop目录分发至集群每台机器
    ```
    scp -r /home/admin/module/HA/hadoop-2.7.2 hd002:~/module/HA/
    scp -r /home/admin/module/HA/hadoop-2.7.2 hd003:~/module/HA/
    ```
    至此 namenode高可用搭建完成,但是不是自动故障转移,切换namenode的active状态需要手动

- ###### 编辑yarn-site.xml(此处为非自动故障转移的配置)
    ```
      <!-- 指定YARN的ResourceManager的地址 -->
      <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hd002</value>
        </property>
        <!-- 日志聚集功能使能 -->
        <property>
                <name>yarn.log-aggregation-enable</name>
                <value>true</value>
        </property>
        <!-- 日志保留时间设置7天 7*24*60*60 -->
        <property>
                <name>yarn.log-aggregation.retain-seconds</name>
                <value>604800</value>
        </property>

        <!-- 日志储存地址 -->
        <property>
                <name>yarn.log.server.url</name>
                <value>http://hd001:19888/jobhistory/logs</value>
        </property>
    ```

- ###### 编辑 slaves
    ```
    hd001
    hd002
    hd003
    ```

- ##### namenode高可用测试
    - ###### 首先启动集群journalnode集群服务
        ```
        #需要启动每个节点的 journalnode 
        $ cd /home/admin/module/HA/hadoop-2.7.2
        $ sbin/hadoop-daemon.sh start journalnode
        $ jps
            7684 Jps
            7609 JournalNode
        ```
    - ###### 格式化namenode
        <font color='red'>注意: 为了使journalnode服务记录namenode初始信息,在格式化namenode前,需要先将journalnode集群服务启动,否则会失败</font>
        ```
        $ bin/hdfs namenode -format
        #  Storage directory /home/admin/module/HA/hadoop-2.7.2/data/tmp/dfs/name has been successfully formatted.
        #  出现这句话时,表示format成功
        ```
    - ###### 启动nn1的namenode
        ```
        sbin/hadoop-daemon.sh start namenode
        ```

    - ###### nn2同步nn1 的元数据信息
        ```
        bin/hdfs namenode -bootstrapStandby

        ```
    - ###### 启动nn2的namenode
        ```
        sbin/hadoop-daemon.sh start namenode

        ```
    - ###### 启动各个节点的datanode
        ```
        # 每隔节点都需要启动 hd001 hd002 hd003
        $ sbin/hadoop-daemon.sh start datanode 

        $ sh ~/jpsutil.sh
        =========== admin@hd001 ============
        11010 DataNode
        11400 Jps
        10286 JournalNode
        =========== admin@hd002 ============
        9744 Jps
        9473 DataNode
        9317 NameNode
        8959 JournalNode
        =========== admin@hd003 ============
        8881 DataNode
        8388 JournalNode
        9161 Jps
        8746 NameNode

        #hd002,hd003中的namenode启动成功
        # 此时 访问 http://hd002:50070/  和 http://hd003:50070/  均发现两个节点处于 standy状态
        # 访问http://hd002:50070/explorer.html#/ 文件目录会提示时: 
        Operation category READ is not supported in state standby
        因为没有active的name造成的

        ```
    - ###### 手动激活nn1中的namenode到active
        ```
        bin/hdfs haadmin -transitionToActive nn1
        #此时再访问 http://hd002:50070/dfshealth.html#tab-overview 显示节点处于active
        访问 http://hd002:50070/explorer.html#/ 可以正常访问
        ```
    - ###### 手动激活nn2中的namenode到active
        ```
        bin/hdfs haadmin -transitionToActive nn2
        Automatic failover is enabled for NameNode at hd002/192.168.1.21:9000
        Refusing to manually manage HA state, since it may cause
        a split-brain scenario or other incorrect state.
        If you are very sure you know what you are doing, please
        specify the --forcemanual flag.

        如果使用
        bin/hdfs haadmin -transitionToActive nn2  --forcemanual
        会提示 nn1已经处于active 不会被切换,切换失败
        18/04/16 18:56:57 WARN ha.HAAdmin: Proceeding with manual HA state management even though
        automatic failover is enabled for NameNode at hd002/192.168.1.21:9000
        transitionToActive: Node nn1 is already active
        Usage: haadmin [-transitionToActive [--forceactive] <serviceId>]
    
        ```
    - ###### 手动激活nn1中的namenode到standby
        ```
        bin/dfs haadmin -transitionToStandby nn1
        # 切换完后后,再切换nn2到active可成功切换
        ```

#### HA的自动故障转移
        以上测试了手动故障转移的方式,下面配置自动故障转移<br>
        HA的自动故障转移依赖于zookeeper

- ##### 配置zookeeper
    ```
    tar -zxvf zookeeper-3.4.10.tar.gz ~/module/HA/
    cd ~/module/HA/zookeeper-3.4.10/
    mkdir zkData
    cd zkData
    echo 1 > myid
    cd ../conf/
    mv zoo_simple.cfg zoo.cfg
    vim zoo.cfg
    # 修改dataDir 
    dataDir=/home/admin/module/HA/zookeeper-3.4.10/zkData
    #末尾增加
    server.1=hd001:2888:3888
    server.2=hd002:2888:3888
    server.3=hd003:2888:3888

    #保存退出
    #将 zookeeper分发到各个节点,修改zkData下myid文件中的值,于主机名称对应
    scp -r ~/module/HA/zookeeper-3.4.10  hd002:~/module/HA/
    scp -r ~/module/HA/zookeeper-3.4.10  hd003:~/module/HA/

    ```
至此,zookeeper配置完成
- ##### zookeeper启动测试
    ```
    #每台节点启动zookeeper服务
    cd /home/admin/module/HA/zookeeper-3.4.10/
    bin/zkServer.sh start
    #查看进程
    [admin@hd001 zookeeper-3.4.10]$ sh ~/jpsutil.sh
    =========== admin@hd001 ============
    14452 Jps
    14390 QuorumPeerMain
    13033 DataNode
    10286 JournalNode
    =========== admin@hd002 ============
    11785 DataNode
    13291 Jps
    11164 NameNode
    13230 QuorumPeerMain
    8959 JournalNode
    =========== admin@hd003 ============
    12817 QuorumPeerMain
    8388 JournalNode
    10892 NameNode
    12879 Jps
    #在其中一台启动zkCli.sh连接服务
    ls /

    ```

- ##### hdfs-site.xml增加配置
    ```
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    ```
- ##### core-site增加配置
    在 core-site.xml 增加
    ```
        <!-- zookeeper通信客户端地址  -->
        <property>
        <name>ha.zookeeper.quorum</name>
        <value>hd001:2181,hd002:2181,hd003:2181</value>
        </property>
    ```
    故障自动转移完成
- ##### 故障自动转移测试
        - 先停掉节点<br>
        sbin/stop-dfs.sh 

        - 初始化HA在Zookeeper中状态：<br>
        bin/hdfs zkfc -formatZK

        - 启动journalnode<br>
        sbin/hadoop-daemon.sh start journalnode

        - 启动dfs<br>
        sbin/start-dfs.sh <br>
        查看hd002:50070和hd003:50070,其中一台处于active
        
        - kill active namenode <br>
        sbin/hadoop-daemon.sh stop namenode <br>
        查看hd002:50070和hd003:50070,刚刚处于standby的namoenode自动切换为active了.


#### 配置yarn-resourcemanager高可用
- 编辑yarn-site.xml
    > 可参考 [Resourcemanager-HA官方文档](http://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html){:target="_blank"}

    ```
    <configuration>
        <!-- reducer获取数据的方式 -->
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>

        <!--启用resourcemanager ha-->
        <property>
            <name>yarn.resourcemanager.ha.enabled</name>
            <value>true</value>
        </property>

        <!--声明两台resourcemanager的地址-->
        <property>
            <name>yarn.resourcemanager.cluster-id</name>
            <value>cluster-yarn1</value>
        </property>

        <property>
            <name>yarn.resourcemanager.ha.rm-ids</name>
            <value>rm1,rm2</value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname.rm1</name>
            <value>hd001</value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname.rm2</name>
            <value>hd002</value>
        </property>

        <!--指定zookeeper集群的地址-->
        <property>
            <name>yarn.resourcemanager.zk-address</name>
            <value>hd001:2181,hd001:2181,hd001:2181</value>
        </property>

        <!--启用自动恢复-->
        <property>
            <name>yarn.resourcemanager.recovery.enabled</name>
            <value>true</value>
        </property>

        <!--指定resourcemanager的状态信息存储在zookeeper集群-->
        <property>
            <name>yarn.resourcemanager.store.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        </property>
        <!-- 日志聚集功能使能 -->
        <property>
                <name>yarn.log-aggregation-enable</name>
                <value>true</value>
        </property>
        <!-- 日志保留时间设置7天 -->
        <property>
                <name>yarn.log-aggregation.retain-seconds</name>
                <value>604800</value>
        </property>

        <property>
                <name>yarn.log.server.url</name>
                <value>http://hd001:19888/jobhistory/logs</value>
        </property>
    </configuration>
    ```
配置完成,分发文件

- rm1启动yarn集群
    sbin/start-yarn.sh

- rm2启动resourcemanager
    sbin/yarn-daemon.sh start resourcemanager <br>
        <font color='red'>注意:<br>
            1.rm2的resourcemanager不会同yarn集群一起启动,需要单独启动;<br>
            2.yarn的resourcemanager的HA,不会经过ZKFC控制,是通过yarn集群自己进行自动切换的</font>  

- 查看集群进程
    ``` 
    sh ~/jpsutil.sh
    [admin@hd001 hadoop-2.7.2]$ sh ~/jpsutil.sh
    =========== admin@hd001 ============
    27937 ResourceManager
    24390 QuorumPeerMain
    27160 JournalNode
    27513 NodeManager
    28026 Jps
    26894 DataNode
    =========== admin@hd002 ============
    27505 ResourceManager
    27271 DFSZKFailoverController
    28232 Jps
    27659 NodeManager
    22556 QuorumPeerMain
    27116 JournalNode
    26988 DataNode
    26861 NameNode
    =========== admin@hd003 ============
    25202 JournalNode
    25074 DataNode
    24968 NameNode
    25595 NodeManager
    22092 QuorumPeerMain
    26044 Jps
    25357 DFSZKFailoverController
    ```
可访问 rm1 和 rm2, hd002:8088  hd003:8088<br>
本文中 rm2 hd002处于activ中, 当访问hd001:8088时,会自动重定向到 hd002:8088/cluster<br>
当将 rm2 停止 后,rm1处于active中<br>

- #### 附件
    - ##### 图解hadoop-HA
    ![](https://stone-upyun.b0.upaiyun.com/blog20180416172754.png!700x999)
    - ##### jpsutil.sh
    ``` bash
        #!/bin/bash
        for i in admin@hd001 admin@hd002 admin@hd003
        do
                echo "=========== $i ============"
                ssh $i 'jps'
        done 
    ```



- #### hadoop-HA[官方文档地址](http://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html){:target="_blank"}